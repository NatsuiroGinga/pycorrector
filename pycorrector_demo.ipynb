{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatsuiroGinga/pycorrector/blob/master/pycorrector_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 文本纠错"
      ],
      "metadata": {
        "id": "yU6QhgtAumUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 下载与导入"
      ],
      "metadata": {
        "id": "4Qb6r6g-u1D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas transformers datasets peft accelerate pycorrector -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U_iv85aGisU",
        "outputId": "3f103bcb-de03-472c-d4ef-18574e03fc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Collecting pycorrector\n",
            "  Downloading pycorrector-1.1.2.tar.gz (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from pycorrector) (0.42.1)\n",
            "Collecting pypinyin (from pycorrector)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pycorrector) (1.17.0)\n",
            "Collecting loguru (from pycorrector)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pyahocorasick (from pycorrector)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycorrector\n",
            "  Building wheel for pycorrector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorrector: filename=pycorrector-1.1.2-py3-none-any.whl size=4364348 sha256=3297fe3032bb4bce86e7dd6f554a7ec94e99eb916f60e4b416a402bd6b872f3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/d0/e5/3f1808b06567ab80b3491c476d9ba9d9714a3283bd56c308a0\n",
            "Successfully built pycorrector\n",
            "Installing collected packages: xxhash, pypinyin, pyahocorasick, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, dill, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, pycorrector\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "24Mbo44_xPTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM\n",
        "from pycorrector import MacBertCorrector\n",
        "from pycorrector.gpt.gpt_corrector import GptCorrector"
      ],
      "metadata": {
        "id": "9dRjOH01wJuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Gradio"
      ],
      "metadata": {
        "id": "xb-w2MO_w43f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    return model.correct(text)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = MacBertCorrector()\n",
        "\n",
        "    examples = [\n",
        "        ['真麻烦你了。希望你们好好的跳无'],\n",
        "        ['少先队员因该为老人让坐'],\n",
        "        ['机七学习是人工智能领遇最能体现智能的一个分知'],\n",
        "        ['今天心情很好'],\n",
        "        ['他法语说的很好，的语也不错'],\n",
        "        ['他们的吵翻很不错，再说他们做的咖喱鸡也好吃'],\n",
        "    ]\n",
        "\n",
        "    gr.Interface(\n",
        "        predict,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"Chinese Spelling Correction Model\",\n",
        "        description=\"Copy or input error Chinese text. Submit and the machine will correct text.\",\n",
        "        article=\"Link to github: <a href='https://github.com/shibing624/pycorrector' style='color:blue;' target='_blank\\'>pycorrector</a>\",\n",
        "        examples=examples\n",
        "    ).launch()"
      ],
      "metadata": {
        "id": "EPJ4QXiKwtMv",
        "outputId": "483862b6-5540-41d4-e845-3c69902c01d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MacBertCorrector' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-979b5b0bcc23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMacBertCorrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     examples = [\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MacBertCorrector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 MacBert-CSC"
      ],
      "metadata": {
        "id": "kvhyP-6kwKC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"shibing624/macbert4csc-base-chinese\")\n",
        "model = BertForMaskedLM.from_pretrained(\"shibing624/macbert4csc-base-chinese\")\n",
        "model.to(device)\n",
        "\n",
        "texts = [\"C请介绍一下\", \"你找到你最喜欢的工作，我也很高心。\"]\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenizer(texts, padding=True, return_tensors='pt').to(device))\n",
        "\n",
        "result = []\n",
        "for ids, text in zip(outputs.logits, texts):\n",
        "    _text = tokenizer.decode(torch.argmax(ids, dim=-1), skip_special_tokens=True).replace(' ', '')\n",
        "    corrected_text = _text[:len(text)]\n",
        "    print(text, ' => ', corrected_text)\n",
        "    result.append(corrected_text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYZn9xJ3E-tX",
        "outputId": "15b66bb5-9c92-4981-aa83-51b4f1c2d4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C请介绍一下  =>  c请介绍一下\n",
            "你找到你最喜欢的工作，我也很高心。  =>  你找到你最喜欢的工作，我也很高兴。\n",
            "['c请介绍一下', '你找到你最喜欢的工作，我也很高兴。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 GPT/Qwen"
      ],
      "metadata": {
        "id": "NYgIUeieE8qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "# 系统角色指令： 你是一名专业的中文文本纠错模型，能够识别并纠正文本中的错别字、多字、漏字，以及常见词汇和领域术语的误用。\n",
        "\n",
        "# 对模型的主要要求：\n",
        "\n",
        "1. 请专注于纠正输入文本中的语言错误，尽量保持原意。\n",
        "\n",
        "2. 当出现领域专有名词（如公司名称、产品名称、技术术语等）时，请参照以下上下文信息进行纠正。\n",
        "\n",
        "3. 对于没有明显错误或不影响理解的部分，不要进行改动。\n",
        "\n",
        "4. 仅输出纠正后的文本，不要输出任何额外解释或标记。\n",
        "\n",
        "# 上下文信息（示例）：\n",
        "\n",
        "1. 真视通：公司名，与“真通智用”“整通只用” 等易混淆\n",
        "\n",
        "2. 紫荆视通：公司名，与“紫金系统”等相近但不相同\n",
        "\n",
        "3. 双相液冷板：正确技术名词，与“双像叶冷板”“双向叶冷板” 相混淆\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "9BeY9g6iV05F",
        "outputId": "247f5f82-5397-4bc0-b134-f761ec971b34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# 系统角色指令： 你是一名专业的中文文本纠错模型，能够识别并纠正文本中的错别字、多字、漏字，以及常见词汇和领域术语的误用。\n",
            "\n",
            "# 对模型的主要要求：\n",
            "\n",
            "1. 请专注于纠正输入文本中的语言错误，尽量保持原意。\n",
            "\n",
            "2. 当出现领域专有名词（如公司名称、产品名称、技术术语等）时，请参照以下上下文信息进行纠正。\n",
            "\n",
            "3. 对于没有明显错误或不影响理解的部分，不要进行改动。\n",
            "\n",
            "4. 仅输出纠正后的文本，不要输出任何额外解释或标记。\n",
            "\n",
            "# 上下文信息（示例）：\n",
            "\n",
            "1. 真视通：公司名，与“真通智用”“整通只用” 等易混淆\n",
            "\n",
            "2. 紫荆视通：公司名，与“紫金系统”等相近但不相同\n",
            "\n",
            "3. 双相液冷板：正确技术名词，与“双像叶冷板”“双向叶冷板” 相混淆\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = GptCorrector()\n",
        "print(m.correct_batch(sentences=['请下真是通的主要业务'], prefix_prompt=prompt))"
      ],
      "metadata": {
        "id": "ygY334LrZOR3",
        "outputId": "cb3e1cd0-b537-45c7-c520-1db3553a4577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GptCorrector' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a4821e335b1e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGptCorrector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'请下真是通的主要业务'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GptCorrector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据增强"
      ],
      "metadata": {
        "id": "beV1HYu7pItN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 下载与导入"
      ],
      "metadata": {
        "id": "uJihTIZBpNIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jionlp"
      ],
      "metadata": {
        "id": "gvIdeD7ApKbg",
        "outputId": "11424106-c950-492c-f0bc-f8d93f33ca7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jionlp\n",
            "  Downloading jionlp-1.5.19-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from jionlp) (1.26.4)\n",
            "Collecting jiojio (from jionlp)\n",
            "  Downloading jiojio-1.2.7-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from jionlp) (2.32.3)\n",
            "Collecting zipfile36 (from jionlp)\n",
            "  Downloading zipfile36-0.1.3-py3-none-any.whl.metadata (736 bytes)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from jiojio->jionlp) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->jionlp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->jionlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->jionlp) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->jionlp) (2025.1.31)\n",
            "Downloading jionlp-1.5.19-py2.py3-none-any.whl (19.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiojio-1.2.7-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl (85.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: zipfile36, jiojio, jionlp\n",
            "Successfully installed jiojio-1.2.7 jionlp-1.5.19 zipfile36-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/Heehobino/transtrue_text_correction"
      ],
      "metadata": {
        "id": "lCrNA5xep1sY",
        "outputId": "00d78e25-8042-44db-d3d8-ea775338f737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transtrue_text_correction'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 22 (delta 9), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (22/22), 14.51 KiB | 1.81 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jionlp as jio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from huggingface_hub import HfApi"
      ],
      "metadata": {
        "id": "sTUUBx1upSOt",
        "outputId": "f523a6e5-60b2-46c1-c1b7-69ca3cd64671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# jionlp - 微信公众号: JioNLP  Github: `https://github.com/dongrixinyu/JioNLP`.\n",
            "# jiojio - `http://www.jionlp.com/jionlp_online/cws_pos` is available for online trial.\n",
            "# jiojio - Successfully load C funcs for CWS and POS acceleration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 同音字替换"
      ],
      "metadata": {
        "id": "SHrVWDzkpU42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "homophone_substitution_config = {\n",
        "    \"input_file_path\": \"/content/transtrue_text_correction/ground_truth.txt\",\n",
        "    \"output_file_path\": \"/content/transtrue_text_correction/train.txt\",\n",
        "    \"augmentation_num\": 20,\n",
        "    \"homo_ratio\": 0.5,\n",
        "    \"allow_mispronounce\": True,\n",
        "}\n",
        "\n",
        "print(jio.homophone_substitution.__doc__)"
      ],
      "metadata": {
        "id": "vCyUSOTAqjj5",
        "outputId": "264c76fe-d18b-4159-d38e-7cf37873c892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    采用同音词汇进行原文替换，达到数据增强的目的。\n",
            "\n",
            "    原理简述：汉语输入法中，拼音输入法为目前使用最广泛的一种打字法，使用率占比约 97%。\n",
            "        在实际使用中，常常出现同音词的打字错误，例如：原句为\n",
            "        “人口危机如果无法得到及时解决，80后、90后们将受到巨大的冲击”，拼音输入法结果为\n",
            "        “人口危机如果无法得到即时解决，80后、90后门将受到巨大的冲击”。\n",
            "        从输入的错误来看，完全不影响人的阅读理解。\n",
            "        因此，可以利用同音词汇替换，达到数据增强的目的。\n",
            "\n",
            "        该工具中，方法具体实施时：\n",
            "        1、不考虑拼音声调，因为拼音输入法基本不输入声调；\n",
            "        2、考虑常见方言读音误读，如 zh 与 z 不分，eng 与 en 不分，f 与 h 不分，l 与 n 不分等情况；\n",
            "        3、替换时，优先使用常用词汇（依据词频而定）；原因在于拼音输入法优先以常见词进行替换。\n",
            "\n",
            "    Args:\n",
            "        text(str): 原始文本\n",
            "        augmentation_num(int): 数据增强对该条样本的扩展个数，默认为 3\n",
            "        homo_ratio(float): 对每一个词汇的同音词替换概率，默认为 0.02\n",
            "        allow_mispronounce(bool): 是否允许方言读音误读，如 zh 与 z 卷舌不分，默认为 True，允许词汇错音\n",
            "        seed(int): 控制随机替换词汇每次不变，默认为 1，当为 0 时，每次调用产生结果不固定\n",
            "\n",
            "    Returns:\n",
            "        list(str): 数据增强的结果，特殊情况可以为空列表\n",
            "\n",
            "    Examples:\n",
            "        >>> import jionlp as jio\n",
            "        >>> res = jio.homophone_substitution(\n",
            "                      '中国驻英记者一向恪守新闻职业道德，为增进中英两国人民之间的了解和沟通发挥了积极作用。')\n",
            "        >>> print(res)\n",
            "\n",
            "        # ['中国驻英记者一向刻手信问职业道德，为增进中英两国人民之间的了解和沟通发挥了积极作用。',\n",
            "        #  '中国驻英记者一向恪守新闻职业道德，为增进中英两国人民指尖的了解和沟通发挥了积极作用。',\n",
            "        #  '中国驻英记者一向恪守新闻职业道德，为增进中英两国人民之间的了解和沟通发挥了积积作用。']\n",
            "\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 测试"
      ],
      "metadata": {
        "id": "vWmsmKvkw9TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 原始真实数据\n",
        "ground_truth = \"请介绍下真通智用公司\"\n",
        "# 生成同音词替换后的数据\n",
        "res = jio.homophone_substitution(ground_truth, augmentation_num=5, homo_ratio=0.5)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "ME0YMa7Cpyw9",
        "outputId": "c245391d-54a1-4dff-8ddb-b3f838da354d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['请介绍吓真通智用公示', '请介绍侠政通支用公司', '请介绍下阵痛智用公司', '请介绍下镇痛只用公示', '轻介绍下真通只用公司']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 并行生成数据"
      ],
      "metadata": {
        "id": "BbqpNMVgxASS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_line(line):\n",
        "    return jio.homophone_substitution(line, augmentation_num=homophone_substitution_config[\"augmentation_num\"], homo_ratio=homophone_substitution_config[\"homo_ratio\"])\n",
        "\n",
        "with open(homophone_substitution_config[\"input_file_path\"], \"r\", encoding=\"utf-8\") as infile, open(homophone_substitution_config[\"output_file_path\"], \"w\", encoding=\"utf-8\") as outfile:\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "        results = executor.map(process_line, infile)  # 并发处理每行数据\n",
        "        for r in results:\n",
        "          outfile.writelines(r)  # 一次性写入文件"
      ],
      "metadata": {
        "id": "5Rb7Kl8AwfBv"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 随机增删字符"
      ],
      "metadata": {
        "id": "3fz4JA0m9Erd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jio.random_add_delete.__doc__)"
      ],
      "metadata": {
        "id": "R_qru28Z9SoJ",
        "outputId": "f07989f4-7c49-45fe-f798-021e60c2c259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 随机增删字符。\n",
            "    随机在文本中增加、删除某个字符。\n",
            "\n",
            "    原理简述：在文本中随机增加、删除某些不影响原意的字符，对文本语义不造成影响。\n",
            "        例如：“23日，山东省监狱管理局原副局长王文杰等5人玩忽职守”，增删为\n",
            "             \"2日，山东监狱 管理局、原副局长文杰等5人玩忽职守..\"。\n",
            "        随机增加的字符的选择，依据对海量文本统计字符分布规律的 char_distribution.json\n",
            "        文件得到，取其中的非中文字符进行添加；该分布经过了修饰，修饰方法参考\n",
            "        self._prepare 方法内的注释。\n",
            "\n",
            "    注意事项：\n",
            "        1、对于某些 NLP 任务，如抽取其中时间词汇，则以上方法很容易干扰关键时间信息，\n",
            "          故方法失效。待后续优化，引入控制参数，避免某类关键信息（时间、地点等被增删）。\n",
            "        2、除了增删外，有一种同义词替换，本工具未采用，原因在于对语言的通畅性与语义影响\n",
            "          过大，几乎找不到可用的增强文本。\n",
            "          例如：“这个东西是干什么用的？”，根据同义词词林，“东西”的同义词包括“家伙”、“货色”、\n",
            "               “小崽子”、“杂种”等，“这个“ 的同义词包括”此“、”斯“等。随机替换后的结果会出现\n",
            "               非常离谱的文本，如 ”斯小崽子是干什么用的？“。\n",
            "          经统计，同义词替换方法的语法不连贯与语义不明确比例占总数据量的 85%，\n",
            "          因此本工具不采用同义词替换方法。\n",
            "\n",
            "    Args:\n",
            "        augmentation_num(int): 数据增强对该条样本的扩展个数，默认为 3\n",
            "        seed(int): 控制随机交换位置每次不变，默认为 1，当为 0 时，每次调用产生结果不固定\n",
            "        add_ratio(float): 对每一个位置随机增加字符概率，默认为 0.02\n",
            "        delete_ratio(float): 对每一个汉字随机做删除的概率，默认为 0.02\n",
            "\n",
            "    Returns:\n",
            "        list(str): 数据增强的结果，特殊情况可以为空列表\n",
            "\n",
            "    Examples:\n",
            "        >>> import jionlp as jio\n",
            "        >>> res = jio.random_add_delete('孙俪晒11年对比照庆领证纪念日，邓超被指沧桑。')\n",
            "        >>> print(res)\n",
            "\n",
            "        # ['孙俪晒11年对比照庆领证纪念日，邓超被指沧。',\n",
            "        #  '孙+俪晒11年对比照庆领证纪念日，邓超被指沧桑。',\n",
            "        #  '孙俪晒 11年对比照庆领证纪念日，邓超被指沧/桑。']\n",
            "\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_add_delete_config = {\n",
        "    \"input_file_path\": \"/content/transtrue_text_correction/ground_truth.txt\",\n",
        "    \"output_file_path\": \"/content/transtrue_text_correction/train.txt\",\n",
        "    \"augmentation_num\": 10,\n",
        "    \"add_ratio\": 0.1,\n",
        "    \"delete_ratio\": 0.1,\n",
        "}"
      ],
      "metadata": {
        "id": "WKHomI2G9bdD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = jio.random_add_delete('请介绍下真通智用公司', augmentation_num=random_add_delete_config[\"augmentation_num\"],\n",
        "add_ratio=random_add_delete_config[\"add_ratio\"], delete_ratio=random_add_delete_config[\"delete_ratio\"])\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "0eDVrGnF9HOt",
        "outputId": "a5af904a-89ee-4672-ba20-cbe8800dd954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['请D介3绍下真通ใ智用公司', '请2绍a下真通用公司', '请介下真y通智用公s司', '请介绍下真智用公']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 回译数据增强"
      ],
      "metadata": {
        "id": "Jq_Ep0McAY0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(baidu_api.__doc__)  # 查看接口说明\n",
        "\n",
        "baidu_api = jio.BaiduApi(\n",
        "        [{'appid': '20250217002276622',\n",
        "          'secretKey': 'dpCCwxOhV3uqFR76aLp7'}], gap_time=0.5)\n",
        "\n",
        "apis = [baidu_api]\n",
        "back_trans = jio.BackTranslation(mt_apis=apis)"
      ],
      "metadata": {
        "id": "hGA9_CgDA-Gr",
        "outputId": "8fba5bdb-83fe-428b-c464-4483481252eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 百度翻译 api 的调用接口\n",
            "\n",
            "    参考文档：https://api.fanyi.baidu.com/doc/21\n",
            "    支持语言：中文(zh)、英文(en)、西班牙语(spa)、德文(de)、法语(fra)、\n",
            "            日语(jp)、俄语(ru)、葡萄牙语(pt)\n",
            "    Args:\n",
            "        from_lang: 输入源语言\n",
            "        to_lang: 输入目标语言\n",
            "\n",
            "    Return:\n",
            "\n",
            "    Examples:\n",
            "        >>> baidu_api = BaiduApi(\n",
            "                [{'appid': '20200618000498778',\n",
            "                 'secretKey': 'raHalLakgYitNuzGOoBZ'}])\n",
            "        >>> text = '她很好看。'\n",
            "        >>> res = baidu_api(text, from_lang='zh', to_lang='en')\n",
            "        >>> print(res)\n",
            "\n",
            "        # She looks good.\n",
            "\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '请介绍下真视通'\n",
        "result = back_trans(text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "SFDgTmOdAeb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}